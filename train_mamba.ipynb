{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install tuned-lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip uninstall -y tuned-lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9cRsIdK2-Jm"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install plotly\n",
        "!pip install ipywidgets\n",
        "!pip install nbformat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGd2YmyD28lk",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import sys\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "import mamba_ssm\n",
        "#sys.path.append('tuned-lens/') <- local tuned-lens with the required changes\n",
        "from tuned_lens.nn.lenses import TunedLens, LogitLens\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from tuned_lens.scripts import train_loop\n",
        "from tuned_lens.scripts import ingredients\n",
        "from pathlib import Path\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers.configuration_utils import PretrainedConfig\n",
        "from transformers import PreTrainedModel\n",
        "from collections import namedtuple\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "\n",
        "#this is so ugly i want to cry\n",
        "@dataclass\n",
        "class mambaConfig:\n",
        "\n",
        "    d_model: int = 768\n",
        "    n_layer: int = 24\n",
        "    vocab_size: int = 50280\n",
        "    ssm_cfg: dict = None\n",
        "    rms_norm: bool = True\n",
        "    residual_in_fp32: bool = True\n",
        "    fused_add_norm: bool = True\n",
        "    pad_vocab_size_multiple: int = 8\n",
        "\n",
        "\n",
        "class MambaConfig(PretrainedConfig):\n",
        "    model_type = \"mamba\"\n",
        "    attribute_map = {\"max_position_embeddings\": \"context_length\"}\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dmodel=768,\n",
        "        vocab_size=50280,\n",
        "        n_layer=24,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = dmodel\n",
        "        self.num_hidden_layers = n_layer\n",
        "        \n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "activations = {}\n",
        "class MambaModel(PreTrainedModel):\n",
        "    config_class = MambaConfig\n",
        "    base_model_prefix = \"model\"\n",
        "    name_or_path = \"mamba\"\n",
        "    name = \"mamba-130m\"\n",
        "    def activation_hook(self, module,input, output):\n",
        "        if len(output)>1:\n",
        "            output = output[0]\n",
        "        activations[module] = output\n",
        "    \n",
        "    def __init__(self, config: MambaConfig):\n",
        "        super().__init__(config)\n",
        "        self.tokenizer=tokenizer\n",
        "        self.dmodel = config.hidden_size\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.num_hidden_layers = config.num_hidden_layers   \n",
        "        self.model = MambaLMHeadModel(mambaConfig)\n",
        "        self.model.name_or_path = \"mamba\"\n",
        "    def load_state_dict(self, state_dict, strict=False):\n",
        "        self.model.load_state_dict(state_dict, strict=strict)\n",
        "\n",
        "    def hook_intermediate(self):\n",
        "        activation_hook = self.activation_hook\n",
        "        self.model.backbone.embedding.register_forward_hook(activation_hook)\n",
        "        for layer in self.model.backbone.layers:\n",
        "            layer.register_forward_hook(activation_hook)\n",
        "    def forward(self, input_ids, output_hidden_states=True):\n",
        "        activations.clear()\n",
        "        if output_hidden_states==True:\n",
        "            self.hook_intermediate()\n",
        "        outputs = self.model(input_ids).logits\n",
        "        hidden_states=[]\n",
        "        for layer in activations.keys():\n",
        "            hidden_states.append(activations[layer])\n",
        "        hidden_states=hidden_states\n",
        "        CausalLMOutput = namedtuple(\"CausalLMOutput\", [\"logits\", \"hidden_states\"])\n",
        "        return CausalLMOutput(logits=outputs, hidden_states=hidden_states)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.model.lm_head\n",
        "\n",
        "    def load(self,device):\n",
        "        return self.to(device), self.tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "model = MambaLMHeadModel.from_pretrained(\"state-spaces/mamba-130m\", device=\"cuda\", dtype=torch.float16)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        "config = MambaConfig(dmodel=768, vocab_size=50280, n_layer=24)\n",
        "mamba= MambaModel(config)\n",
        "mamba.load_state_dict(model.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_data = ingredients.Data([\"roneneldan/TinyStories\"])\n",
        "optimizer = ingredients.Optimizer()\n",
        "distributer = ingredients.Distributed(per_gpu_batch_size=1)\n",
        "p = Path(\"next\")\n",
        "loss=train_loop.LossChoice.KL\n",
        "train_data.split = \"train\"\n",
        "train_data.text_column=\"text\"\n",
        "train = train_loop.Train(mamba,train_data,optimizer,distributer,p,wandb=\"Lens\",loss=loss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train.execute()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tuned_lens = TunedLens.from_model(mamba).cuda()\n",
        "\n",
        "state = torch.load(\"future/params.pt\")\n",
        "tuned_lens.layer_translators.load_state_dict(state)\n",
        "logit_lens = LogitLens.from_model(mamba).cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL8f4i2828lm",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from tuned_lens.plotting import PredictionTrajectory\n",
        "import ipywidgets as widgets\n",
        "from plotly import graph_objects as go\n",
        "\n",
        "\n",
        "def make_plot(lens, text, layer_stride, statistic, token_range):\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    targets = input_ids[1:] + [tokenizer.eos_token_id]\n",
        "\n",
        "    if len(input_ids) == 0:\n",
        "        return widgets.Text(\"Please enter some text.\")\n",
        "    \n",
        "    if (token_range[0] == token_range[1]):\n",
        "        return widgets.Text(\"Please provide valid token range.\")\n",
        "    pred_traj = PredictionTrajectory.from_lens_and_model(\n",
        "        lens=lens,\n",
        "        model=mamba,\n",
        "        input_ids=input_ids,\n",
        "        tokenizer=tokenizer,\n",
        "        targets=targets,\n",
        "    ).slice_sequence(slice(*token_range))\n",
        "\n",
        "    return getattr(pred_traj, statistic)().stride(layer_stride).figure(\n",
        "        title=f\"{lens.__class__.__name__} ({mamba.name_or_path}) {statistic}\",\n",
        "    )\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "statistic_wdg = widgets.Dropdown(\n",
        "    options=[\n",
        "        ('Entropy', 'entropy'),\n",
        "        ('Cross Entropy', 'cross_entropy'),\n",
        "        ('Forward KL', 'forward_kl'),\n",
        "    ],\n",
        "    description='Select Statistic:',\n",
        "    style=style,\n",
        ")\n",
        "text_wdg = widgets.Textarea(\n",
        "    description=\"Input Text\",\n",
        "    value=\"it was the best of times, it was the worst of times\",\n",
        ")\n",
        "lens_wdg = widgets.Dropdown(\n",
        "    options=[('Tuned Lens', tuned_lens), ('Logit Lens', logit_lens)],\n",
        "    description='Select Lens:',\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "layer_stride_wdg = widgets.BoundedIntText(\n",
        "    value=2,\n",
        "    min=1,\n",
        "    max=10,\n",
        "    step=1,\n",
        "    description='Layer Stride:',\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "token_range_wdg = widgets.IntRangeSlider(\n",
        "    description='Token Range',\n",
        "    min=0,\n",
        "    max=1,\n",
        "    step=1,\n",
        "    style=style,\n",
        ")\n",
        "\n",
        "\n",
        "def update_token_range(*args):\n",
        "    token_range_wdg.max = len(tokenizer.encode(text_wdg.value))\n",
        "\n",
        "update_token_range()\n",
        "\n",
        "token_range_wdg.value = [0, token_range_wdg.max]\n",
        "text_wdg.observe(update_token_range, 'value')\n",
        "\n",
        "interact = widgets.interact.options(manual_name='Run Lens', manual=True)\n",
        "\n",
        "plot = interact(\n",
        "    make_plot,\n",
        "    text=text_wdg,\n",
        "    statistic=statistic_wdg,\n",
        "    lens=lens_wdg,\n",
        "    layer_stride=layer_stride_wdg,\n",
        "    token_range=token_range_wdg,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ca27fca65bbd5c56c827a2643e94bc7b2b551ee6ee2fe84566f2c789012bce4f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
